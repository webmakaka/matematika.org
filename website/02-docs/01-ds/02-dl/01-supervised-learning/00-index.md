---
layout: page
title: Обучение с учителем (Supervised Learning) применительно к DeepLearning
description: Обучение с учителем (Supervised Learning) применительно к DeepLearning
keywords: Обучение с учителем, Supervised Learning, DeepLearning
permalink: /ds/dl/supervised-learning/
---

# Обучение с учителем (Supervised Learning) применительно к DeepLearning

- Наблюдаемые величины (наблюдения, observations) — сущности, поведение которых мы хотим предсказать. Наблюдаемые величины обозначаются x. Иногда мы будем называть их входными данными (inputs).

- Целевые переменные (targets) — соответствующие наблюдаемым величинам метки. Обычно именно их мы и предсказываем. Следуя общепринятой нотации в машинном/глубоком обучении, мы будем их обозначать y. Иногда эти метки называют контрольными значениями (ground truth).

- Модель представляет собой математическое выражение или функцию, принимающую на входе наблюдаемую величину x и предсказывающую значение целевой метки.

- Параметры — иногда называются также весами и служат для параметризации модели. Их обычно обозначают w (от англ. weights) или ŵ.

- Предсказания, называемые также оценками (estimates), представляют собой значения целевых переменных, предсказанные моделью по наблюдаемым величинам. Для их обозначения мы будем использовать «шляпку». Так, предсказание целевой переменной y обозначается ŷ.

- Функция потерь (loss function) — это функция, служащая мерой отклонения предсказания от целевой переменной для наблюдений из обучающей последовательности. Функция потерь ставит целевой переменной и ее предсказанию в соответствие скалярное вещественное значение, называемое потерями (loss). Чем меньше значение потерь, тем лучше модель предсказывает целевую переменную. Мы будем обозначать функцию потерь L.

<br/>

### Про градиентный спуск

Задача машинного обучения с учителем состоит в поиске значений параметров, минимизирующих функцию потерь для данного набора данных. Другими словами, это эквивалентно поиску корней уравнения. Как известно, градиентный спуск (gradient descent) — распространенный метод поиска корней уравнения. Напомним, что в обычном методе градиентного спуска выбираются какие-либо начальные значения для корней (параметров), после чего они обновляются в цикле до тех пор, пока
вычисленное значение целевой функции (функции потерь) не окажется ниже заданного порогового значения (критерий сходимости). Для больших наборов данных реализация обычного градиентного спуска — задача почти неразрешимая вследствие ограничений памяти и очень медленно работающая из-за вычислительных издержек. Вместо этого обычно используется аппроксимация градиентного спуска, называемая стохастическим градиентным спуском (stochastic gradient descent, SGD). При этом случайным образом выбирается точка данных (или подмножество точек данных), и для заданного подмножества вычисляется градиент. В случае отдельной точки данных такой подход называется чистым SGD, а в случае подмножества (из более чем одной) точек данных — мини-пакетным SGD. Эпитеты «чистый» и «мини-пакетный» обычно опускают, если используемый вариант метода понятен из контекста. На практике чистый SGD применяется редко из-за его очень медленной сходимости
вследствие шума.

Существует множество вариантов общего алгоритма SGD, нацеленных на ускорение сходимости. Алгоритмы SGD используются при обновлении значений параметров. Этот процесс итеративного обновления значений параметров называется методом обратного распространения ошибки (backpropagation). Каждый шаг (так называемая эпоха) алгоритма обратного распространения ошибки состоит из прямого прохода (forwardpass) и обратного прохода (backward pass). При прямом проходе выполняется вычисление наблюдаемых величин при текущих значениях параметров и рассчитывается функция потерь. На обратном шаге значения параметров обновляются на основе градиента потерь.

<br/>

Взято:

[Брайан Макмахан, Делип Рао] Знакомство с PyTorch: глубокое обучение при обработке естественного языка [RUS, 2020]
