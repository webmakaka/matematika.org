---
layout: page
title: Машинное обучения без учителя (unsupervised learning)
description: Машинное обучения без учителя (unsupervised learning)
keywords: Машинное обучения без учителя, unsupervised learning
permalink: /books/ds/ml/ru/scikit-learn/introduction-to-ml-with-python/unsupervised-learning/
---

# Машинное обучения без учителя (unsupervised learning)

https://github.com/amueller/introduction_to_ml_with_python/blob/master/03-unsupervised-learning.ipynb

Машинное обучение без учителя включает в себя все виды машинного обучения, когда ответ неизвестен и отсутствует учитель, указывающий ответ алгоритму. В машинном обучении без учителя есть лишь входные данные
и алгоритму необходимо извлечь знания из этих данных.

Мы рассмотрим 2 вида обучения без учителя:

- Неконтролируемые преобразования (unsupervised transformations)
- Алгоритмы кластеризации (clustering algorithms)

<br/>

### Неконтролируемые преобразования (unsupervised transformations)

**Анализ главных компонент (principal component analysis, PCA)** - осуществляет вращение данных с тем, чтобы преобразованные признаки не коррелировали между собой. Часто это вращение сопровождается выбором подмножества новых признаков в зависимости от их важности с точки зрения интерпретации данных.

**Факторизация неотрицательных матриц(non-negative matrix factorization, NMF)** - выделяет полезные характеристики. В NMF нам нужно получить неотрицательные компоненты и коэффициенты, то есть нам нужны компоненты и коэффициенты, которые больше или равны нулю. Поэтому этот метод может быть применен только к тем данным, в которых характеристики имеют неотрицательные значения.

**Алгоритмамы множественного обучения (manifold learning algorithms)**. Особенно полезным является алгоритм t-SNE (t-distributed stochastic neighbor embedding, t-SNE). Идея, лежащая в основе алгоритма t-SNE, заключается в том, чтобы найти двумерное представление данных, сохраняющее расстояния между точками наилучшим образом.

t-SNE начинает свою работу со случайного двумерного представления каждой точки данных, а затем пытается сблизить точки, которые в пространстве исходных признаков находятся близко друг к другу, и отдаляет друг от друга точки, которые находятся далеко друг от друга. При этом t-SNE уделяет большее внимание сохранению расстояний между точками, близко расположенными друг к другу. Иными словами, он пытается сохранить информацию, указывающую на то, какие точки являются соседями друг другу.

<br/>

### Алгоритмы кластеризации (clustering algorithms)

**Кластеризация k-средних**

Сначала выбирается число кластеров k . После выбора значения k алгоритм k -средних отбирает точки, которые будут представлять центры кластеров (cluster centers). Затем для каждой точки данных вычисляется его евклидово расстояние до каждого центра кластера. Каждая точка назначается ближайшему центру кластера. Алгоритм вычисляет центроиды ( centroids ) – центры тяжести кластеров. Каждый центроид – это вектор, элементы которого представляют собой средние значения характеристик, вычисленные по всем точкам кластера. Центр кластера смещается в его центроид. Точки заново назначаются ближайшему центру кластера. Этапы изменения центров кластеров и переназначения точек итеративно повторяются до тех пор, пока границы кластеров и расположение центроидов не перестанут изменяться, т.е. на каждой итерации в каждый кластер будут попадать одни и те же точки данных.

Получив новые точки данных, алгоритм k-средних будет присваивать каждую точку данных ближайшему центру кластера.

**Алгомеративная кластеризация (agglomerative clustering)**. Алгоритм начинает свою работу с того, что каждую точку данных заносит в свой собственный кластер и по мере выполнения объединяет два наиболее схожих между собой кластера до тех пор, пока не будет удовлетворен определенный критерий остановки.

Критерий остановки, реализованный в scikit-learn – это количество
кластеров, поэтому схожие между собой кластеры объединяются до тех
пор, пока не останется заданное число кластеров. Есть несколько
критериев связи ( linkage ), которые задают точный способ измерения
«наиболее схожего кластера». В основе этих критериев лежит расстояние
между двумя существующими кластерами.

**DBSCAN (density-based spatial clustering of applications with noise – плотностный алгоритм кластеризации пространственных данных с присутствием шума)**. Основные преимущества алгоритма DBSCAN заключаются в том, что пользователю не нужно заранее задавать количество кластеров, алгоритм может выделить кластеры сложной формы и способен определить точки, которые не принадлежат какому-либо кластеру. DBSCAN работает немного медленнее, чем алгоритм агломеративной кластеризации и алгоритм k -средних, но также может масштабироваться на относительно большие наборы данных.

DBSCAN определяет точки, расположенные в «густонаселенных» областях пространства характеристик, когда многие точки данных расположены близко друг к другу. Эти области называются плотными ( dense ) областями пространства характеристик. Идея алгоритма DBSCAN заключается в том, что кластеры образуют плотные области данных, которые отделены друг от друга относительно пустыми областями.

Точки, находящиеся в плотной области, называются ядровыми примерами ( core samples ) или ядровыми точками ( core points ). Алгоритм DBSCAN имеет два параметра: min_samples и eps. Если по крайней мере min_samples точек находятся в радиусе окрестности eps рассматриваемой точки, то эта точка классифицируется как ядровая. Ядровые точки, расстояния между которыми не превышают радиус окрестности eps, помещаются алгоритмом DBSCAN в один и тот же кластер. На старте алгоритм выбирает произвольную точку. Затем он находит все точки, удаленные от стартовой точки на расстоянии, не превышающем радиус окрестности eps. Если множество точек, находящихся в пределах радиуса окрестности eps, меньше значения min_samples, стартовая точка помечается как шум ( noise ), это означает, что она не принадлежит какому-либо кластеру. Если это множество точек больше значения min_samples, стартовая точка помечается как ядровая и ей назначается метка нового кластера. Затем посещаются все соседи этой точки (находящиеся в пределах eps). Если они еще не были присвоены кластеру, им присваивается метка только что созданного кластера. Если они являются ядровыми точками, поочередно посещаются их соседи и т.д. Кластер растет до тех пор, пока не останется ни одной ядерной точки в пределах радиуса окрестности eps. Затем выбирается другая точка, которая еще не была посещена, и повторяется та же самая процедура.

В итоге получаем три вида точек: ядровые точки, точки, которые находятся в пределах радиуса окрестности eps ядровых точек (так называемые пограничные точки или boundary points ) и шумовые точки. При многократном применении алгоритма DBSCAN к конкретному набору данных результаты кластеризации ядровых точек будут всегда одинаковыми, при этом одни и те же точки всегда будут помечаться как шумовые. Однако пограничная точка может быть соседом для ядровых точек из нескольких кластеров. Поэтому кластерная принадлежность пограничных точек зависит от порядка посещения точек. Как правило, существует лишь несколько пограничных точек, поэтому эта слабая зависимость результатов кластеризации от порядка посещения точек не имеет значения.
